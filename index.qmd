---
title: "Dossier: Analyzing data analysis in R"
author: "H. Sherry Zhang " 
institute: "University of Texas at Austin" 
date: "2025 Jun 05"
format: 
  revealjs:
    scrollable: true
    slide-number: true
    show-slide-number: all
    aspectratio: 169
    theme: serif
    preview-links: auto
    pdf-separate-fragments: true
    footer: "https://sherryzhang-lmu2025.netlify.app"
preamble: >
  \usepackage{amsfonts,amsmath,amssymb,amsthm}
title-slide-attributes: 
  data-background-image: figures/logo.png
  data-background-size: 7%
  data-background-position: 98% 98%
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r setup}
#| include: false  
library(knitr)
library(readr)
library(tidyverse)
library(text)
library(dossier)
library(ggdendro)
library(ggrepel)
library(patchwork)
source("helper.R")
options(htmltools.dir.version = FALSE)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
               error = FALSE, fig.align = "center")
```



Imagine your research assistant/ student enters into a new domain and needs to analyze data.

Most common way is to ask them to read a few papers in the domain, and they can grasp the methods and data analysis from the papers. 

But we all know it's manual work ... what if we can encode the literature as a database and query it? 

Then it takes a much less time to understand more papers in the domain, maybe we can trace them by time, compare methods used by the authors, and cluster papers by the methods they used.

It's like treating data analysis decisions as data to analyze them. 

--- 

## {.smaller background-image="figures/overview.png" background-size="60%" background-position="50% 50%"}

---

## {.smaller background-image="figures/decision.png" background-size="80%" background-position="50% 50%"}

<!-- ![](figures/decision.png){width="100%"} -->

---

## {.smaller background-image="figures/decision2.png" background-size="80%" background-position="50% 50%"}

---

## {.smaller background-image="figures/decision3.png" background-size="80%" background-position="50% 50%"}

---

## Decision table {.smaller}

| Paper |  Variable | Method | Parameter | Type | Reason | Decision| 
|-------|------|-------|--------|-----------|--------|---------|
| Ostro | time | smoothing spline | degree of freedom | parameter | (Samet et al. 2000) | 7 df per year | 
| Ostro | temperature | smoothing spline | degree of freedom | parameter | NA | 3 df | 
| Ostro | temperature | NA | NA | temporal | NA | 1-day lag | 
| Ostro | humidity | smoothing spline | degree of freedom | parameter | NA | 3 df | 
| Ostro | humidity | NA | NA | temporal | NA | 1-day lag | 


---

## Automatic reading decisions with LLM 

::: {.scroll-container}
![](figures/llm-prompt.png){width="100%"}
:::

---

## Text similarity to compare decisions

```{r}
sentence1 <- "3 df"
sentence2 <- "50% of the data"
sentence3 <- "7 df"
model <- textEmbed(texts = c(sentence1, sentence2, sentence3))

# cosine similarity by default
textSimilarity(x = model$texts$texts[1,], y = model$texts$texts[2,])
textSimilarity(x = model$texts$texts[2,], y = model$texts$texts[3,])
textSimilarity(x = model$texts$texts[1,], y = model$texts$texts[3,])
```


---

## [Clustering and find paper similarities]{.r-fit-text}{.smaller} 

`run_hclust`, `run_mds`, and `ggplot2`

```{r echo = FALSE}
#| fig-height: 5.5
paper_df <- read_rds("data/papers_df.rds")

# calculate decision similarity and paper similarity
embed_df <- paper_df |> compute_text_embed()
distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
distance_df <- distance_decision_df |> calc_paper_similarity()

# cluster the paper by hierarchical clustering and multi-dimensional scaling
hclust_res <- run_hclust(distance_df, "ave")
mds_res <- run_mds(distance_df)

# find the most common method for each paper for plotting
method_df <- paper_df |> get_most_common_method() # from `helper.R`
hclust_res$labels <- hclust_res$labels |> left_join(method_df, by = c("label" = "paper"))
mds_df <- mds_res |> left_join(method_df, by = "paper")

# plot the dendrogram and multi-dimensional scaling
p2 <- ggplot() +
  geom_segment(data = segment(hclust_res), aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(data = hclust_res$labels,
            aes(x = x, y = y, label = label, color = method, hjust = 0),
            size = 3) +
  coord_flip() +
  theme_void() +
  scale_color_brewer(palette = "Dark2") +
  scale_y_reverse(expand = c(0.2, 0))

p3 <- mds_df |> ggplot(aes(x = V1, y = V2)) +
  ggrepel::geom_text_repel(aes(label = paper, color = method)) +
  theme_bw() +
  theme(aspect.ratio = 1) +
  scale_color_brewer(palette = "Dark2") +
  ggtitle("Multi-dimensional scaling")

(p2 | p3) & theme(legend.position = "bottfom") + 
  theme(plot.background = element_rect(fill = '#F0F1EB', colour = '#F0F1EB'),
        panel.background = element_rect(fill = '#F0F1EB', colour = '#F0F1EB'))

```

--- 

## {.smaller background-image="figures/your-time.png" background-size="65%" background-position="50% 50%"}





<!-- papers --(LLM)-> decision database --(analysis)-> insights -->

<!-- what's a decision  -->
<!--   - an example  -->

<!-- the designed table: parameter, spatial, and temporal  -->

<!-- read in the paper via LLM (show the prompt) -->

<!-- output the decision database (show the table) -->

<!-- clean  -->

<!-- Calculate paper similarity based on decisions  -->

<!-- text similarity on the reason and decisions  -->

<!-- aggregate decisions similarities to paper similarities -->

<!-- show visuals  -->
